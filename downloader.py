#!/usr/bin/python3
"""
downloader.py – Cookidoo recipe downloader

Reads a JSON file containing recipe URLs (generated by planner.py), removes
duplicates and downloads each recipe page + image locally, following (and
re-using) most of cookidump.py logic.
"""

import os
import io
import re
import json
import time
import argparse
from pathlib import Path
from urllib.request import urlretrieve
from urllib.parse import urlparse

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service

from bs4 import BeautifulSoup

# ---------------------------------------------------------------------------
PAGELOAD_TO = 3
SCROLL_TO   = 1

# ---------------------------------------------------------------------------
# Utilities copied / adapted from cookidump.py
# ---------------------------------------------------------------------------

def start_browser(chrome_driver_path):
    chrome_options = Options()
    if "GOOGLE_CHROME_PATH" in os.environ:
        chrome_options.binary_location = os.getenv('GOOGLE_CHROME_PATH')
    # chrome_options.add_argument('--headless=new')
    chrome_service = Service(chrome_driver_path)
    return webdriver.Chrome(service=chrome_service, options=chrome_options)


def img_to_file(outputdir, recipe_id, img_url):
    img_path = Path(outputdir) / 'images' / f'{recipe_id}.jpg'
    img_path.parent.mkdir(parents=True, exist_ok=True)
    urlretrieve(img_url, str(img_path))
    # relative path used inside saved HTML
    return f'../images/{recipe_id}.jpg'


def recipe_to_file(browser, filename):
    Path(filename).parent.mkdir(parents=True, exist_ok=True)
    html = browser.page_source
    with io.open(filename, 'w', encoding='utf-8') as f:
        f.write(html)


def recipe_to_json(browser, recipe_id):
    soup = BeautifulSoup(browser.page_source, 'html.parser')
    recipe = {
        'id': recipe_id,
        'language': soup.select_one('html').attrs.get('lang', ''),
        'title': soup.select_one('.recipe-card__title').text if soup.select_one('.recipe-card__title') else '',
        'rating_count': re.sub(r'\D', '', soup.select_one('.core-rating__label').text, flags=re.IGNORECASE) if soup.select_one('.core-rating__label') else '',
        'rating_score': soup.select_one('.core-rating__counter').text if soup.select_one('.core-rating__counter') else ''
    }
    # add more fields if desired
    return recipe

# ---------------------------------------------------------------------------


def main():
    parser = argparse.ArgumentParser(description='Download Cookidoo recipes from a list of URLs (produced by planner.py)')
    parser.add_argument('webdriver', type=str, help='Path to Chrome WebDriver')
    parser.add_argument('urls_json', type=str, help='Path to JSON file containing recipe URLs')
    parser.add_argument('outputdir', type=str, help='Directory where HTML/IMG/JSON will be stored')
    parser.add_argument('-s', '--separate-json', action='store_true', help='Write one JSON per recipe (default: aggregate)')
    args = parser.parse_args()

    urls = json.loads(Path(args.urls_json).read_text(encoding='utf-8'))
    urls = [u for u in urls if 'recipe' in u]
    unique_urls = sorted(set(urls))
    print(f'[DOWNLOADER] {len(unique_urls)} unique recipe URLs loaded.')

    brw = start_browser(args.webdriver)

    # Ask user to login once
    base_search_url = 'https://cookidoo.com.cn/search/zh-Hans-CN?languages=zh'
    brw.get(base_search_url)
    time.sleep(PAGELOAD_TO)
    input('[DOWNLOADER] 请登录 Cookidoo，然后回到终端按 Enter 继续…')

    outputdir = args.outputdir.rstrip('/') + '/'

    recipe_data = []
    for idx, recipe_url in enumerate(unique_urls, start=1):
        recipe_id = recipe_url.split('/')[-1]
        try:
            brw.get(recipe_url)
            time.sleep(PAGELOAD_TO)

            # attempt to remove <base> that messes relative links
            try:
                brw.execute_script("var el=document.querySelector('base');if(el){el.remove();}")
            except Exception:
                pass

            # download and relink recipe image
            try:
                img_url = brw.find_element(By.ID, 'recipe-card__image-loader').find_element(By.TAG_NAME, 'img').get_attribute('src')
                local_img = img_to_file(outputdir, recipe_id, img_url)
                brw.execute_script("document.querySelector('.core-tile__image').src=arguments[0];", local_img)
            except Exception:
                pass

            recipe_to_file(brw, f"{outputdir}recipes/{recipe_id}.html")
            data = recipe_to_json(brw, recipe_id)
            if args.separate_json:
                with open(f"{outputdir}recipes/{recipe_id}.json", 'w', encoding='utf-8') as fh:
                    json.dump(data, fh, ensure_ascii=False)
            else:
                recipe_data.append(data)

            if idx % 10 == 0:
                print(f'[DOWNLOADER] Saved {idx}/{len(unique_urls)} recipes')
        except Exception as exc:
            print(f'[DOWNLOADER] !!! Failed to download {recipe_url}: {exc}')

    if not args.separate_json:
        with open(f"{outputdir}data.json", 'w', encoding='utf-8') as fh:
            json.dump(recipe_data, fh, ensure_ascii=False)

    print('[DOWNLOADER] All done – closing browser.')
    brw.quit()


if __name__ == '__main__':
    main() 